{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT: Bidirectional Encoder Representation from Transformers(对Transformer的双向编码进行调整后的算法)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artical: BERT(https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优点\n",
    "1. BERT拥有一个深而窄的神经网络，由于是无监督学习，因此不需要人工干预和标注，让低成本地训练超大规模语料成为可能\n",
    "2. BERT模型能够联合神经网络所有层中的上下文来进行训练\n",
    "3. BERT只需要微调就可以适应很多类型的NLP任务\n",
    "4. BERT 模型只通过添加一个额外的输出层来进行微调，就能够创建出 state-of-the-art 模型用于各种不同的任务4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT 是第一个基于微调的表征模型，它在大量的语句级和 token 级任务中实现了最先进的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤\n",
    "1. 首先，将语料中的某一部分词汇遮盖住，让模型根据上下文双向预测被遮盖的词，来初步训练出通用模型\n",
    "2. 从语料中挑选出连续的上下文语句，让transformer模型来识别这些语句是否连续\n",
    "3. 这两步合在一起完成预训练，就成为一个能够实现上下文全向预测出的语言表征模型\n",
    "4. 再结合精加工（fine tuning）模型，使之适用于具体应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Language Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing\n",
    "Vectorizing text is the process of transforming text into numeric tensor\n",
    "### n-grams\n",
    "Word n-grams are groups of N consecutive words that you can extract from a sentence\n",
    "### word embedding\n",
    "Source:(https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html)\n",
    "其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NLP Popular Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source(CN): https://www.jianshu.com/p/c59ae92a7a27  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Feature-based Approaches\n",
    "\n",
    "word embedding / sentence embedding / paragraph embedding  \n",
    "这些学习特征通常作为特征被用于下游模型  \n",
    "Example: ELMo:将传统的 word embedding 推广至另一个维度。他们提出了从语言模型中提取文本敏感（context-sensitive）的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Fine-tuning Approaches\n",
    "在 LM 目标上预先训练一些模型架构，然后对监督下游任务的相同模型进行微调  \n",
    "Example: Generative Pre-trained Transformer(GPT)引入极小的任务特定的参数，并对其进行训练通过简单地微调方法预训练参数来完成下游任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Transfer Learning from Supervised Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
