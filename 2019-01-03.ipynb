{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priori vs. Posteriori\n",
    "Source(CH): https://blog.csdn.net/yangang908/article/details/62215209  \n",
    "Priori:事件发生前的预判概率。可以是基于历史数据的统计，可以由背景常识得出，也可以是人的主观观点给出。一般都是单独事件概率，如P(x),P(y)。  \n",
    "Posteriori: 事件发生后求的反向条件概率；或者说，基于先验概率求得的反向条件概率。概率形式与条件概率相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机采样方法整理  \n",
    "Source(CN):https://blog.csdn.net/xbinworld/article/details/43612641  \n",
    "Source(CN): http://www.cnblogs.com/daniel-D/p/3388724.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Integral \n",
    "Source(CN): https://www.qiujiawei.com/monte-carlo/  \n",
    "蒙特卡洛积分:对一个连续函数的采样方法是在该函数的定义域中随机挑N个值，并求出对应的N个f(Xi)，就得到了样本集合。再对这些样本集合做一些换算，就可以得到一个近似的积分了\n",
    "![title](Figure/Mento_Carlo.png)\n",
    "其中pdf是指probability distribution function,对于连续函数f，f的每个可能取值x的出现概率等于x的取值范围[a,b]的倒数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-Muller \n",
    "Source(CN): https://blog.csdn.net/baimafujinji/article/details/6492982  \n",
    "如果随机变量 U1,U2 独立且U1,U2∼Uniform[0,1]，\n",
    "![title](Figure/Box_Muller.png)\n",
    "则 Z0,Z1 独立且服从标准正态分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo principle\n",
    "X 表示随即变量，服从概率分布p(x), 那么要计算f(x)的期望，只需要我们不停从p(x)中抽样xi，然后对这些f(xi)取平均即可近似f(x)的期望。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acceptance-Rejection sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然 p(x) 太复杂在程序中没法直接采样，那么我设定一个程序可抽样的分布 q(x) 比如高斯分布，然后按照一定的方法拒绝某些样本，达到接近 p(x) 分布的目的，其中q(x)叫做 proposal distribution (参考分布)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Figure/Accept_Rejection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Figure/Accept_Rejection_Algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC Algorithm\n",
    "Source(EN): An introduction to MCMC for machine learning, Andrieu, Christophe  \n",
    "MCMC 的绝妙之处在于：通过稳态的 Markov Chain 进行转移计算，等效于从 P(x) 分布采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MM Trasitiion\n",
    "![WTHA](Figure/mcmc-transition.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis-Hastings Algorithm\n",
    "\n",
    "![title](Figure/MHAlgorithm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling\n",
    "![2D](Figure/Gibbs_Sampling_Algorithm.jpg)  \n",
    "![ALL](Figure/Gibbs_Sampling.png)\n",
    "Gibbs Sampling 一大特点是没有接受概率，因此状态转移总能实行，比MH有更快的收敛速度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gibbs Sampling方法来训练rbm会非常慢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Divergence 对比散度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CD的训练过程中用到了Gibbs 采样，即在训练过程中，首先将可视向量值映射给隐单元，然后用隐层单元重建可视向量，接着再将可视向量值映射给隐单元……反复执行这种步骤。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
