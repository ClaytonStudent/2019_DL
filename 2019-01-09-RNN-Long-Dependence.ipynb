{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 长期依赖问题\n",
    "Source(CN): https://zhuanlan.zhihu.com/p/34490114  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 门机制\n",
    "1. LSTM\n",
    "2. GRU\n",
    "3. Minimal Gated Unit\n",
    "4. Simple Recurrent Unit\n",
    "5. QRNN\n",
    "6. Recurrent Additive Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 LSTM and GRU\n",
    "Source(EN): https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Another View of LSTM\n",
    "Source(CN): https://zhuanlan.zhihu.com/p/34500721"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Minimal Gated Unit\n",
    "ARTICAL:Minimal Gated Unit for Recurrent Neural Networks(https://arxiv.org/pdf/1603.09420.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Simple Recurrent Unit\n",
    "ARTICAL: Training RNNs as Fast as CNNs (https://arxiv.org/pdf/1709.02755.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 QRNN: Quasi-Recurrent Neural Networks\n",
    "ARTICAL: Quasi-Recurrent Neural Networks(https://arxiv.org/pdf/1611.01576.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Recurrent Additive Networks\n",
    "ARTICAL: Recurrent Additive Networks(http://kentonl.com/pub/llz.2017.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 跨尺度连接\n",
    "1. CW-RNN: Clockwise RNN\n",
    "2. Dilated RNN\n",
    "3. NARX RNN\n",
    "4. TKRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 CW-RNN: Clockwise RNN\n",
    "把隐层分成很多组，每组有不同的循环周期，有的周期是 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dilated RNN\n",
    "在不同的层分组：最下面的隐层每个时间步都循环，较高的隐层循环周期更长些，从而有效感受野更大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 NARX RNN\n",
    "ARTICAL: Learning Long-Term Dependencies in NARX Recurrent Neural Networks(http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1032.9079&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 TKRNN\n",
    "ARTICAL: Temporal Kernel Recurrent Neural Networks(http://www.cs.utoronto.ca/~ilya/pubs/2008/tkrnn.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 特殊初始化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
