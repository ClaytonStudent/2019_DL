{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read: Attention is all you need\n",
    "Source(CN): https://zhuanlan.zhihu.com/p/36699992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Problem\n",
    "This inherently sequential nature precludes parallelization within training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Model: factorization tricks / conditional computation\n",
    "\n",
    "Other models: Extended Neural GPU / ByteNet / ConvS2S   \n",
    "1. all of which use convolutional neural networks as basic building block  \n",
    "2. the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions(In the Transformer this is reduced to a constant number of operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Attention\n",
    "Source(CN): https://zhuanlan.zhihu.com/p/37601161   \n",
    "深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似,例如英德翻译是，对不同的德语单词在英文句子理所有单词的重要行不同，即注意力不同。  \n",
    "大多数注意力模型附着在Encoder-Decoder框架下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Soft Attention\n",
    "理解Attention模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Attention](Figure/Attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Attention Calculation \n",
    "![Attention](Figure/Attention_Algorithm_three.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Attention Algorithm\n",
    "Summary:  \n",
    "![Attention_0](Figure/Attention_Algorithm.png)\n",
    "Step1:计算target单词和source句子中所有单词的相似性  \n",
    "![Attention_1](Figure/Attention_Similarity.jpg)\n",
    "Step2:用softmax分配概率\n",
    "![Attention_2](Figure/Attention_Softmax.png)\n",
    "Step3:加权求和得出Attention值\n",
    "![Attention_3](Figure/Attention_Sum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Self-Attention(intra-attention)\n",
    "\n",
    "Article: Why Attention Model(https://arxiv.org/pdf/1808.08946.pdf)  \n",
    "Source(CN): https://www.jiqizhixin.com/articles/2018-09-17-5  \n",
    "relating different positions of a single sequence in order to compute a representation of the sequence（关联单个序列的不同位置以计算序列的表示）  \n",
    "指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。  \n",
    "引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征(句法特征，语义特征)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Other Sources\n",
    "https://www.jianshu.com/p/c94909b835d6  \n",
    "https://blog.csdn.net/mpk_no1/article/details/72862348  \n",
    "https://www.jianshu.com/p/e14c6a722381  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-end memory networks\n",
    "based on a recurrent attention mechanism instead of sequence- aligned recurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connection\n",
    "Source(CN):https://zhuanlan.zhihu.com/p/28413039"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additive attention vs. dot-product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Residual Network\n",
    "Source(CN): https://zhuanlan.zhihu.com/p/22447440"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
