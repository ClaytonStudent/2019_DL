{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read: Attention is all you need\n",
    "Source(CN)(翻译): https://zhuanlan.zhihu.com/p/36699992  \n",
    "Source(CN)(解读): https://kexue.fm/archives/4765  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RNN vs. Attention\n",
    "\n",
    "### 1.1 RNN Problem\n",
    "This inherently sequential nature precludes parallelization within training examples\n",
    "\n",
    "### 1.2 Solve Methods\n",
    "\n",
    "Original Model: factorization tricks / conditional computation\n",
    "\n",
    "Other models: Extended Neural GPU / ByteNet / ConvS2S   \n",
    "1. all of which use convolutional neural networks as basic building block  \n",
    "2. the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions(In the Transformer this is reduced to a constant number of operations)\n",
    "\n",
    "### 1.3 Transformer Model\n",
    "![Transformer](Figure/Transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extended Neural GPU\n",
    "ARTICAL: Can Active Memory Replace Attention?(https://arxiv.org/pdf/1610.08613.pdf)  \n",
    "ARTICAL: NEURAL GPUS LEARN ALGORITHMS(https://arxiv.org/pdf/1511.08228.pdf)    \n",
    "#### 2.1.1. Neural Turing Machines\n",
    "Source(CN): https://zhuanlan.zhihu.com/p/24150657  \n",
    "核心思想是在RNNs的基础上augment记忆模块\n",
    "#### 2.1.2 Turing completeness\n",
    "Source(CN): https://www.zhihu.com/question/20115374  \n",
    "如果一系列操作数据的规则（如指令集、编程语言、细胞自动机）可以用来模拟单带图灵机，那么它是图灵完全的.  \n",
    "图灵完全性通常指“具有无限存储能力的通用物理机器或编程语言”  \n",
    "意思是在原则上(尽管不是经常在实践上)它能够用来解决任何计算性的问题\n",
    "### 2.2 ByteNet\n",
    "ARTICAL: Neural Machine Translation in Linear Time (https://arxiv.org/pdf/1610.10099.pdf) \n",
    "#### 2.2.1 Dilated Convolution network\"  \n",
    "Source(CN): https://zhuanlan.zhihu.com/p/23795111  \n",
    "Dilated convolution的主要贡献就是，如何在去掉池化下采样操作的同时，而不降低网络的感受野。    \n",
    "### 2.3 ConvS2S\n",
    "ARTICAL: Convolutional Sequence to Sequence Learning (https://arxiv.org/pdf/1705.03122.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Pointwise\n",
    "Source(CN): https://cuiqingcai.com/5019.html  \n",
    "Pointwise 将问题转化为多分类或回归问题。  \n",
    "如果归结为多分类问题，对于某个 Query，对文档与此 Query 的相关程度打标签，标签分为有限的类别，这样就将问题转为多分类问题；  \n",
    "如果归结为回归问题，对于某个 Query，则对文档与此 Query 的相关程度计算相关度 Score，这样就将问题归结为回归问题。  \n",
    "模型: Subset Ranking /OC SVM / McRank / Prank   \n",
    "### 3.2 Residual Connection\n",
    "Source(CN):https://zhuanlan.zhihu.com/p/28413039\n",
    "\n",
    "![Residual](Figure/Residual_Connection.jpg)\n",
    "\n",
    "### 3.3 Position Embedding\n",
    "Source:https://kexue.fm/archives/4765   \n",
    "将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了。\n",
    "Position Embedding是位置信息的唯一来源，也可以提供相对位置信息  \n",
    "公式:\n",
    "![Position_Embedding](Figure/Position_Embedding.png)\n",
    "### 3.4 Multi-Head Attention\n",
    "所谓“多头”（Multi-Head），就是只多做几次同样的事情（参数不共享），然后把结果拼接。\n",
    "\n",
    "### 3.5 Separable Convolutions\n",
    "Source(CN): https://zhuanlan.zhihu.com/p/29367273\n",
    "\n",
    "### 3.6 Deep Residual Network\n",
    "Source(CN): https://zhuanlan.zhihu.com/p/22447440"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
